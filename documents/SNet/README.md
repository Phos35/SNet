# 一、完整模型

![](http://43.138.43.178:8000/images/SNet.drawio.svg)

由上图可以发现，`SNet`并不复杂（至少可以在一张图中完整地表现出核心的功能和完整的处理流程），但要从完整模型中直接入手去分析，也是需要花费一些时间的。以下部分开始从最顶层的抽象逐步向下进行解析，详细地解析从功能表现到实现细节的具体流程。

在深入之前，记一下完整模型中左上角的四类箭头所代表的含义：

1. <font color="#6c8ebf">蓝色→</font>：客户端连接过程
2. <font color="#d6b656">黄色→</font>：客户端发送数据到服务端过程
3. <font color="#b85450">红色→</font>：客户端处理客户端数据过程
4. <font color="#9673a6">紫色→</font>：服务端响应客户端过程



# 二、功能抽象

## 1. 第一层抽象 -- Server、Client

![](http://43.138.43.178:8000/images/Server-Client.drawio.svg)
这是最顶层的抽象。这一部分只要是接触过计网中TCP的，应该都能理解图上这三个流程：

1. 客户端发送连接请求，申请建立连接
2. `TCP`连接建立后，客户端发送数据给服务端，这部分数据可以是多种类型的，比如`HTTP`中这部分数据就是`HTTPReqeust`，而在其他协议中，也可以是对服务端的命令等
3. 服务端在经过对客户端数据的处理后，生成相对应的响应并传回客户端

根据最顶层的功能抽象，我们可以很容易将服务端的功能进行划分，然后做出第二层抽象：

1. 连接的建立
2. 数据的收发

第二层抽象就分别对应了`TCPServer`的数据成员：

1. `Acceptor` —— 负责接收客户端连接请求，建立连接
2. `TCPConnection` —— 负责与客户端的数据交流

来想想看我们为什么要这么划分？
服务端可能会同时接受到数量众多的连接请求，如果将建立连接与数据交流放在一起顺序执行会发生什么？众多连接请求就必须顺序接收，后来的客户端就需要等待漫长的时间才能够连接到服务器。想想我们平时使用网络浏览网站，我们能够忍受看着网页显示服务端拒绝连接，等待网站按照先来先到的顺序逐个处理完前面的所有其他客户端再来接收我们的请求吗？这显然是不能接受的。
因此我们需要将接受连接与数据处理分开，进行并发处理，这样所有进行连接的客户端都能够及时被连接，同时根据TCPConnection的实现策略，所有用户都能够陆陆续续地、平等地得到响应，而不是某个客户端很快就被响应了，而其他客户端就必须等待很长时间才能被处理。



## 2. 第二层抽象——Acceptor，建立连接

![](http://43.138.43.178:8000/images/Acceptor.drawio.svg)
建立连接是服务端与客户端交流的第一步，我们先来看看普通的socket通信流程：

1. 服务端建立代表自身的server_socket（socket()），并为其绑定特定的地址和端口（bind()）,接着便开始监听（listen()）,等待客户端连接请求的到来（accept()）
2. 客户端建立代表自身的client_socket（socket()），然后去连接服务端正在监听的地址（connect()），等到服务端接收连接
3. 服务端在特定地址的端口上监听到客户端的连接请求（accept()），获得代表客户端的client_socket，并告知客户端自己已经接收连接
4. 客户端接收到服务端接收连接的通知，便开始进入接下来的数据通信阶段（事实上这一步还会返回对服务端通知的确认信息，这部分属于TCP三次握手的知识，此处不做细讲）

客户端和服务端建立连接的最基本过程就是如上所述，十分简单。那么接下来看看我们为什么需要一个Acceptor来接收连接，而不是简单地直接使用socket来完成连接的功能：

1. 面向对象地去设计。连接请求这一部分功能很集中，我们很容易想到将相关地功能、数据聚集在一起，形成一个类
2. 隐藏原则。尽管socket创建连接地过程很简单，但是它与我们在最顶层地抽象相比，仍然是过于繁杂了。我们在服务端想要的功能是什么？是：接收连接->获取客户端信息。Acceptor就是为了隐藏socket的具体操作，然后把建立连接、获取客户端信息的接口提供给上层，这样我们就能够依照最直观的顶层抽象来实现我们的程序了



## 3. 第二层抽象——`TCPConnection`，数据的收发

![](http://43.138.43.178:8000/images/TCPConnection.drawio.svg)


`TCPConnection`作为对一次连接的抽象，代表了收发两端与它们之间的通信连接，因此在服务端与客户端建立连接后由它来接管与客户端交流数据的收发，这也同样符合Acceptor设计的两个出发点：面向对象和隐藏。

1. 面向对象，是因为`TCPConnection`的功能集中，因此将其相关数据和方法聚集在一起，形成一个类
2. 隐藏，是因为`TCPConnection`隐藏了底层收发数据需要的read、write、缓冲区等实现细节，而提供了简单的、对应于顶层抽象中接收客户端数据、发送响应的接口。

在图中我们可以看到`TCPConnection`将客户端数据的解析处理交到了外部，想想看这是为什么？
这部分与实现相关：[WorkerPool——数据处理](#3.  ——数据处理)



`SNet`的抽象层次就如上所述，是很简单的模型。实现细节则相对复杂了很多。

# 三、实现细节

## 1. `EventLoop`——事件驱动的核心数据结构

`EventLoop`——事件循环，是事件驱动机制的核心结构，它能够在自己的线程中循环监听事件的发生并通过调用事件的回调函数进行相应的处理。`SNet`中的`Acceptor`与`TCPConnection`都依托于`EventLoop`实现。尽管在实现中还存在`EventLoopTrhead`用来指示封装事件循环的线程，但实际上我们可以将`EventLoop`直接指代事件循环线程，因为`EVentLoop`脱离线程是没有任何意义的（`EventLoop`是一个死循环执行结构，如果不单独绑定线程的话会阻塞调用`EventLoop.run()`的线程，就无法完成其它有意义的工作了）

- 处理流程
  ![](http://43.138.43.178:8000/images/EventLoop_Procedure.drawio.svg)
- `Acceptor`与`EventLoop`
  `Acceptor`独占一个`EventLoop`，其处理的IO事件就是server_socket上到来的连接请求事件。通过`EventLoop`的不断循环监听处理，`Acceptor`就能不断接收新连接，并通过启动时设置的回调函数将新连接递交给`TCPServer`完成新连接的创建
- `TCPConnection`与`EventLoop`
  `TCPConnection`与`EventLoop`之间存在从属关系：
  ![](http://43.138.43.178:8000/images/EventLoop_TCPConnection_Relation.drawio.svg)


每当`TCPServer`创建新的`TCPConnection`，就会将其分配到一个`EventLoop`上，由`EventLoop`监听`TCPConnection`上的事件，当事件出发的时候就调用`TCPConnection`创建时提交的回调函数来处理对应的事件。比如当客户端发来数据的时候，`EventLoop`就会得到该`TCPConnection`上的读事件，紧接着就会调用`TCPConnection`的读处理回调函数来读取客户端的数据



## 2. `IOMutiplexing`——事件驱动的核心机制

`IOMutiplexing`——IO多路复用机制，是一种能够使单个线程监听多个文件描述符事件的机制。

让我们来考虑以下在不使用IO多路复用机制的情况下我们需要如何并发地处理连接：

1. 我们不能在accept()的线程中去处理连接数据。如果我们在处理accept()的线程中去处理数据，我们就需要对接收到的连接进行read()，但是我们并不知道对端何时会发送数据，这种时候就有两个情况
   1. socket使用阻塞方式。这种情况下为了保证我们能处理已连接的所用请求，我们需要对已连接的每个客户端都进行read，然而我们并不知道对端何时会传送数据过来。当我们进行read的socket的客户端长时间不发送数据，我们的服务端就无法进行其他工作——既不能处理已连接客户端发送的数据，也不能接收新的连接
   2. socket使用非阻塞方式。这种情况下为了保证能处理已连接的所有请求，就需要不断地read已连接客户端的socket，然而一次遍历中我们该对每个socket进行read几次呢？实际是几次都不够，因为我们并不知道客户端何时会发送来数据，假设我们在客户端断开连接前无限次的read，那么我们的accept就无法被执行到，新的连接就无法建立；如果我们循环read有限次，那么我们就会回到accept()，假设我们在accept阻塞的过程中已连接客户端发送来了数据，而未来很长时间都不会再有客户端连接到来，那么已连接客户端数据就无法得到处理
2. 对于1，一种可能的解决方法就是对每个到来的连接创建一个处理该连接的线程，由创建的线程来读取、处理、发送数据。乍一听似乎是个很好的解决方法，但我们来实际看看会存在哪些问题：
   1. socket使用非阻塞模式。当socket使用非阻塞模式，负责该socket的县城就需要不断地去read判断对端是否发送了数据。如果对端恰好在短时间内发送了数据并断开连接，那么这种操作还算有效。但是如果对端长时间不发送数据，那这个线程就只是白白浪费CPU去做了无用功
   2. socket使用阻塞模式。socket使用阻塞模式能够解决上面使用非阻塞模式时浪费CPU的问题，它能够在read没有数据时让出CPU，直到有数据到来时才去竞争。然而，这里面还是存在问题的，假设大量客户端连接进来并且都长时间不断开连接，会发生什么？
      这会导致大量的线程被创建。且先不说线程资源是有限的，无法无限制地去创建，这些被创建的线程如果在某一时刻同时启动了大部分，光是线程切换带来的开销都足够降低实际的工作效率，影响对客户端的响应速度了。
      使用线程池的机制，则存在因为已分配的客户端长期不断开连接而占用池中所有线程而无法处理其它客户端数据的问题。

那么再来看看IO多路复用机制如何处理：

1. IO多路复用机制能够监听文件描述符上的事件，只有当其监听的文件描述符中确实又发生事件的时候才会被唤醒并返回触发事件的文件描述符。这就避免了上述阻塞read无法接收新连接、阻塞accept无法处理已有连接数据、不断read socket判断对端是否发送数据带来CPU开销等问题
2. 单个线程足以应付并发的连接。对于IO多路复用，我们可以根据触发事件的文件描述符来选择对应的处理函数，因此我们的监听描述符和客户端连接描述符可以平等对待，就不存在accept阻塞影响read、read阻塞影响accept的情况，这样单线程就足以应付并发的连接。如果使用多线程，可以使用线程池来利用有限的线程来提高处理效率

IO多路复用机制的监听特点是`EventLoop`执行流程中监听的基础：只有有事件发生的时候才会被唤醒，不会因阻塞导致事件无法被处理。



## 3.  `WorkerPool`——数据处理

![](http://43.138.43.178:8000/images/WorkerPool.drawio.svg)

这张图与[TCPConnection](#3. 第二层抽象——`TCPConnection`，数据的收发)中的图是相连的，注意图中的<font color="#b85450">红色→</font>和<font color="#9673a6">紫色→</font>

还记得为什么要将处理客户端数据的工作与`TCPConnection`分离的问题吗？客户端数据处理与TCP Connection分离是基于如下原因的：

1. `TCPConnection`的主要功能是数据的收发，重点在于IO，而客户端数据的解析处理属于CPU任务，这两部分在功能层次上是分离的
2. `TCPConnection`的读写事件处理都在`EventLoop`中进行，而一个`EventLoop`需要负责多个`TCPConnection`的事件处理，如果将数据的处理任务和数据的收发任务集中在一起，那么`EventLoop`中处理每一个`TCPConnection`的读事件就需要更多的时间开销，必然会延误后续`TCPConnection`事件的处理。因此将数据的处理从`TCPConnection`中抽离出来，作为单独的模块与`TCPConnection`的读写并发进行，尽可能提升读写事件的处理效率，加快响应速度

`WorkerPool`是一个结构很简单的线程池，但是其中task_queue使用了`weak_ptr`来作为任务，这是有点费解的，后续在[SNet中的智能指针](.https://github.com/Phos35/SNet/tree/master/documents/RAII)中会添加相应的解释，目前在这里先放置一个TODO作为标记。

`WokerPool`中的关键点在于每个线程调用的`MessageProcessor`，这部分是处理客户端数据的核心，也是`SNet`能为应用层协议框架实现提供接口的基础



## 4.  `MessageProcessor`——扩展性的基础

![](http://43.138.43.178:8000/images/Base_MessageProcessor.drawio.svg)
`SNet`的`MessageProcessor`包含如下的基类：

1. `Message`：报文基类。上层协议中如果有多种报文类型，需要都继承该类
2. `Decoder`：报文解析器。上层根据协议设计的报文解析器`UpperDecoder`需要继承改类
3. `Dispatcher`：消息分发器。上层根据协议设计的消息分发器`UpperDispatcher`需要继承此类
4. `MessageProcessor`：消息处理器。内部包含`Decoder`和`Dispathcer`，上层需要将自己的`UpperDecoder`和`UpperDispatcher`整合进自己定义的`UpperMessageProcessor`，并继承该类。
5. `MessageProcessorFactory`：消息处理器工厂，用于为每个`worker`线程创建`MessageProcessor`。上层需要设计`UpperMessageProcessorFactory`，并重写`create_processor`接口用于创建`UpeerMessageProcessor`

`MessageProcessor`体系中的这些结构均采用了虚函数的方法来将上层的报文解析和消息分发功能转移到了`SNet`的TCP层次，这样上层就只需要专注于如何实现协议，而不需要担心底层的数据的处理流程。

关于`MessageProcessor`如何实现扩展，可以查看[http_demo](.https://github.com/Phos35/SNet/tree/master/documents/http_demo/README.md)，这部分将通过实现一个简单的HTTP服务器来讲解如何利用`SNet`提供的接口实现应用层协议框架



## 5. 主从`Reactor`模型——多线程高并发

- 主从Reactor模型
  ![](http://43.138.43.178:8000/images/Master_Sub_Reactor.drawio.svg)
  主从Reactor模型是解决多线程高并发的有效模型，通过职能划分、线程池等的设计来完成连接请求、数据的高效处理
  职能划分：

  1. Master Reactor：专注于接收连接，将接受到的连接均衡分配到Sub Reactor
  2. Sub Reactor：专注于读写数据，读取客户端请求数据、发送服务端响应数据
  3. Worker：专注于编解码和业务执行

- `SNet`中的主从Reactor模型
  ![](http://43.138.43.178:8000/images/SNet_Master_Sub_Reactor.drawio.svg)

  | 主从Reactor模型 |             SNet             |                  职能区别                  |
  | :-------------: | :--------------------------: | :----------------------------------------: |
  | Master Reactor  |           Acceptor           |                均为接收连接                |
  |   Sub Reactor   | 管理TCPConnection的EventLoop |            均为负责连接读写数据            |
  |     Worker      |            Worker            | SNet是变相的编解码和业务逻辑负责，实质相同 |

  